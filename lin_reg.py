import numpy as np
import scipy.ndimage
import matplotlib.pyplot as plt
from PIL import Image as image
import cPickle
import os
import urllib


class Data_Base(object):
    """
    Create a database object to load images from disk to memory.
    image_source = string of text file name of the dataset, generated by make_data_set
    image_dir = filepath to the directory containing the images named in image_source (since image_source.txt only have
    the name, not the filepath)
    classes = dictionary mapping class names (str) (corresponding to labels of image_source.txt) to numeric values (int)
    return = database object
    """
    def __init__(self, image_source, image_dir, classes):
        self.image_dir = image_dir
        self.image_source = image_source
        self.classes = classes
        self.num_class = len(self.classes)
        self.im_count = self.image_count()
        self.imdb, self.image_size = self.load_images()  # list of tuples (label_as_class_index, image_as_1D_array)

    def image_count(self):
        with open(self.image_source) as f:
            for i, l in enumerate(f):
                pass
        return i+1

    def load_images(self):
        f = open(self.image_source)
        i = 0
        for line in f:
            ndimage = scipy.ndimage.imread(self.image_dir+line.split('\t')[0], True)
            ndimage.resize(ndimage.size)
            ndimage /= 255.0
            label = self.classes[line.split('\t')[1].strip('\n')]
            if i == 0:
                image_db = [(label,ndimage)]
            else:
                image_db = image_db + [(label, ndimage)]
            i += 1
        return image_db, ndimage.size


class Linear_Classifier(object):
    """
    Classifier for 2 class classification using linear regression. Call with database object. call .grad_descent() to
    perform regression. Returns Linear_Classifier object
    """
    def __init__(self, db):
        self.step_size = 0.001
        self.EPS = 1e-5
        self.max_iter = 1e5
        self.report_iter = 500
        self.subtract_image_mean = True
        self.db = db
        self.input_size = db.image_size
        self.linear_weights = self.init_weights()
        self.x, self.y = self.load()
        self.f_t = np.ndarray(int(self.max_iter))

    def init_weights(self): #returns weight matrix
        #linear_weights = scipy.stats.norm.rvs(size = self.input_size+1, random_state=1)
        #linear_weights /= 1024.0
        linear_weights = np.zeros(self.input_size+1)
        return linear_weights

    def forward(self):  #compute and returns hypothesis x*theta
        z = np.dot(self.x, self.linear_weights)
        return z

    def error(self,z):  #compute errors given hypothesis, z is X*theta
        return np.sum(((self.y-z)**2))/(2*len(self.db.imdb))

    def grad(self,z): #compute gradient of error, z is X*theta
        return -1 * np.sum((self.y - z) * self.x.transpose(), 1) / len(self.db.imdb)

    def load(self):   #copy data and labels from database to classifier, append ones to data for bias term
        x = np.zeros((len(self.db.imdb),self.input_size+1,))
        y = np.zeros((len(self.db.imdb)))
        i = 0
        for im in self.db.imdb:
            x[i] = np.insert(im[1], 0, 1.)
            y[i] = im[0]
            i +=1
        if self.subtract_image_mean:
            x[:, 1:] = x[:, 1:] - np.average(x[:, 1:])
        return x, y

    def grad_descent(self): #performs gradient descent
        prev_t = self.linear_weights - 10 * self.EPS
        t = self.linear_weights.copy()
        iter = 0
        while np.linalg.norm(self.linear_weights - prev_t) > self.EPS and iter < self.max_iter:
            prev_t = self.linear_weights.copy()
            z = self.forward()
            error = self.error(z)
            grad = self.grad(z)
            self.linear_weights -= self.step_size * grad
            if iter % self.report_iter == 0:
                print "Iter", iter
                print "f(x) = %.5f" % (error)
                print "Gradient Magnitude: ", np.linalg.norm(grad), "\n"
                self.f_t[int(iter//self.report_iter)] = error
            iter += 1
        self.f_t = self.f_t[:int(iter//self.report_iter)]
        np.save(self.db.image_dir.strip('/')+'_'+self.db.image_source.split('.')[0],self.linear_weights)
        #self.linear_weights = t


class Linear_Classifier_nD(object):
    """N class classifier using linear regression. Call with database object to initialize. Call grad_descent to perform
    gradient descent using current configuration. Returns Linear_Classifier_nD object"""
    def __init__(self, db):
        self.step_size = 0.001
        self.EPS = 5e-5
        self.max_iter = 1e5
        self.report_iter = 500
        self.subtract_image_mean = True
        self.db = db
        self.input_size = db.image_size
        self.linear_weights = self.init_weights()
        self.x, self.y = self.load()
        self.f_t = np.ndarray(int(self.max_iter))

    def init_weights(self): #returns weight matrix
        #Initialize weights to zero
        linear_weights = np.zeros((self.input_size+1,self.db.num_class))
        return linear_weights
        #linear_weights = np.ndarray((self.db.num_class, self.input_size+1))
        #for i in range(0,self.db.num_class):
            #linear_weights[i] = scipy.stats.norm.rvs(size = self.input_size+1,random_state=1)
        #linear_weights /=1024.
        #return linear_weights.transpose()

    def forward(self):  #compute and returns hypothesis x*theta
        z = np.dot(self.x, self.linear_weights)
        return z

    def error(self,z):  #compute error given hypothesis
        return np.sum(np.sum(((self.y-z)**2)))/(2.*len(self.db.imdb))

    def grad(self,z):   #compute gradient given hypothesis
        return np.dot(self.x.transpose(), (z-self.y))/float(len(self.db.imdb))

    def load(self):     #copy data and labels from database to classifier, append ones to data for bias term
        x = np.zeros((len(self.db.imdb),self.input_size+1,))
        y = np.zeros((len(self.db.imdb),self.db.num_class))
        i = 0
        for im in self.db.imdb:
            x[i] = np.insert(im[1], 0, 1.)
            y[i] = im[0]
            i +=1
        if self.subtract_image_mean:
            x[:, 1:] = x[:, 1:] - np.average(x[:, 1:])
        return x, y

    def grad_descent(self): #performs gradient descent
        prev_t = self.linear_weights - 10 * self.EPS
        t = self.linear_weights.copy()
        iter = 0
        while np.linalg.norm(self.linear_weights - prev_t) > self.EPS and iter < self.max_iter:
            prev_t = self.linear_weights.copy()
            z = self.forward()
            error = self.error(z)
            grad = self.grad(z)
            self.linear_weights -= self.step_size * grad
            if iter % self.report_iter == 0:
                print "Iter", iter
                print "f(x) = %.5f" % (error)
                print "Gradient Magnitude: ", np.linalg.norm(grad), "\n"
                self.f_t[int(iter//self.report_iter)] = error
            iter += 1
        self.f_t = self.f_t[:int(iter//self.report_iter)]
        np.save(self.db.image_dir.strip('/')+'_'+self.db.image_source.split('.')[0],self.linear_weights)


def make_data_set(act, set_id, size=100):
    """
    Generate 3 text files containing names of images and label (last name) of each image. The number of images for
    training is equal to the size specified. Testing and validation set size is 10.
    :param act: a list of actor full names as string
    :param set_id: suffix (before extension) of the output text files
    :param size: default 100, size of training set
    :return: none
    """
    all_images = []
    for file in os.listdir("cropped/"):
        if file.lower().endswith(".jpg") or file.lower().endswith(".jpeg") or file.lower().endswith(".png"):
            all_images += [file]

    train_set = open('training_set%s.txt' % (set_id),'w+')
    validation_set = open('validation_set%s.txt' % (set_id),'w+')
    testing_set = open('testing_set%s.txt' % (set_id),'w+')
    count = np.zeros(len(act))
    for image in all_images:
        i = 0
        for actor in act:
            if image[0:4] == actor.split()[1].lower()[0:4]:
                if count[i]<size:
                    train_set.write(image+'\t'+actor.split()[1]+'\n')
                elif count[i]<size+10:
                    validation_set.write(image+'\t'+actor.split()[1]+'\n')
                elif count[i]<size+20:
                    testing_set.write(image+'\t'+actor.split()[1]+'\n')
                count[i] += 1
            i += 1
    train_set.close()
    validation_set.close()
    testing_set.close()

def timeout(func, args=(), kwargs={}, timeout_duration=1, default=None):
    '''From:
    http://code.activestate.com/recipes/473878-timeout-function-using-threading/'''
    import threading
    class InterruptableThread(threading.Thread):
        def __init__(self):
            threading.Thread.__init__(self)
            self.result = None

        def run(self):
            try:
                self.result = func(*args, **kwargs)
            except:
                self.result = default

    it = InterruptableThread()
    it.start()
    it.join(timeout_duration)
    if it.isAlive():
        return False
    else:
        return it.result




print('Part 1')
#download all image data, crop, and turn into grayscale, outputs saved to ./cropped folder, full images stored in ./uncropped folder

act =['Fran Drescher', 'America Ferrera', 'Kristin Chenoweth', 'Alec Baldwin', 'Bill Hader', 'Steve Carell','Gerard Butler', 'Daniel Radcliffe', 'Michael Vartan', 'Lorraine Bracco', 'Peri Gilpin', 'Angie Harmon']
testfile = urllib.URLopener()

try:
    os.mkdir("uncropped/")
except:
    print 'Directory Exists, continueing'

crop_list = open("actors_list.txt", "w+")

for a in act:
    name = a.split()[1].lower()
    i = 0
    for line in open("facescrub_actresses.txt"):
        if a in line:
            filename = name + str(i) + '.' + line.split()[4].split('.')[-1]
            # A version without timeout (uncomment in case you need to
            # unsupress exceptions, which timeout() does)
            # testfile.retrieve(line.split()[4], "uncropped/"+filename)
            # timeout is used to stop downloading images which take too long to download
            timeout(testfile.retrieve, (line.split()[4], "uncropped/" + filename), {}, 30)
            if not os.path.isfile("uncropped/" + filename):
                continue
            crop_list.write(filename+'\t'+line.split('\t')[4]+'\n')
            print filename+'\t'+line.split('\t')[4]+'\n'
            #print filename
            i += 1

crop_list.close()

crop_list = open("actors_list.txt","r")
fromdir = 'uncropped/'
todir = 'cropped/'

try:
    os.mkdir(todir)
except:
    print 'Directory Exists, continuing'

for line in crop_list:
    line = line.strip('\n')
    imname = line.split('\t')[0]
    try:
        Im = image.open(fromdir+imname).convert('L')
    except:
        print "broken image " + imname
        continue
    box = [float(i) for i in line.split('\t')[1].split(',')]
    crop_im = Im.crop(box)
    crop_im = crop_im.resize((32,32))
    crop_im.save(todir+imname)


print('Part 2')
#make the data sets of 100 actors for part 3, outputs text file.
act = ['Bill Hader', 'Steve Carell']
#all_images = os.listdir('cropped/')
all_images = []
for file in os.listdir("cropped/"):
    if file.lower().endswith(".jpg") or file.lower().endswith(".jpeg") or file.lower().endswith(".png"):
        all_images = all_images + [file]


size = 100
train_set = open('training_set.txt', 'w+')
validation_set = open('validation_set.txt', 'w+')
testing_set = open('testing_set.txt', 'w+')
count = np.zeros(len(act))

for image in all_images:
    i = 0
    for actor in act:
        if image[0:4] == actor.split()[1].lower()[0:4]:
            if count[i] < size:
                train_set.write(image + '\t' + actor.split()[1] + '\n')
            elif count[i] < size + 10:
                validation_set.write(image + '\t' + actor.split()[1] + '\n')
            elif count[i] < size + 20:
                testing_set.write(image + '\t' + actor.split()[1] + '\n')
            count[i] += 1
        i += 1
train_set.close()
validation_set.close()
testing_set.close()

print('Part 3')
imdb1 = Data_Base('training_set.txt','cropped/', {'Hader': -1, 'Carell': 1})

classifier1 = Linear_Classifier(imdb1)
classifier1.grad_descent()
'''
with open('linear_classifier1-2.pkl', 'wb') as f:
    cPickle.dump(classifier1,f,protocol=-1)

'''
#with open('linear_classifier.pkl','rb') as f:
    #classifier1 = cPickle.load(f)

#################### Validation Part 3###################################
imdb_val = Data_Base('validation_set.txt','cropped/', {'Hader': int(-1), 'Carell': int(1)})
classifier2 = Linear_Classifier(imdb_val)
classifier2.linear_weights = classifier1.linear_weights.copy()
output_val =classifier2.forward()
loss_val = classifier2.error(output_val)
print('Validation Loss: ')
print(loss_val)

pred_val = np.zeros(output_val.shape, dtype=int)
res = np.zeros(output_val.shape, dtype=int)
gt_label = classifier2.y
for i in range(0,output_val.shape[0]):
    if output_val[i]>=0:
        pred_val[i] = 1
    else:
        pred_val[i] = -1
    res[i] = pred_val[i]==int(gt_label[i])

accuracy = np.sum(np.array(res))/float(len(res))
print('Validation accuracy: ')
print(accuracy)
#Validation Loss:
#2.51565344979
#Validation accuracy:
#0.8

#################### Testing Part 3#######################################
imdb_test = Data_Base('testing_set.txt','cropped/', {'Hader': int(-1), 'Carell': int(1)})
classifier3 = Linear_Classifier(imdb_test)
classifier3.linear_weights = classifier1.linear_weights.copy()
output_val =classifier3.forward()
loss_val = classifier3.error(output_val)
print('Testing Loss: ')
print(loss_val)

pred_val = np.zeros(output_val.shape, dtype=int)
res = np.zeros(output_val.shape, dtype=int)
gt_label = classifier3.y
for i in range(0,output_val.shape[0]):
    if output_val[i]>=0:
        pred_val[i] = 1
    else:
        pred_val[i] = -1
    res[i] = pred_val[i]==int(gt_label[i])

accuracy = np.sum(np.array(res))/float(len(res))
print('Testing accuracy: ')
print(accuracy)
#Validation Loss:
#0.252029348837
#Validation accuracy:
#0.85
#Testing Loss:
#0.188357613313
#Testing accuracy:
#0.95

#################### Part 4 #######################################
print('Part 4')
heatmap = classifier1.linear_weights[1:].reshape((32,32))
plt.imsave('Image_outputs/200image_trained.png',heatmap)
hmap = plt.imshow(heatmap, cmap='gray', interpolation= 'nearest')
#plt.show(hmap)
plt.close('all')

errors = classifier1.f_t
iters = np.linspace(0,errors.size-1,errors.size)
error_descent = plt.plot(iters,errors)
plt.ylabel('Error')
plt.xlabel('Iteration')
plt.savefig('Image_outputs/Classifier1_training lossn.png')
plt.close('all')
with open('4imgset.txt','wb') as f:
    f.write('carell44.jpg\tCarell\n'
            'carell39.jpg\tCarell\n'
            'hader48.jpg\tHader\n'
            'hader132.jpg\tHader\n')

imdb4 = Data_Base('4imgset.txt','cropped/',{'Hader': int(-1), 'Carell': int(1)})
classifier4 = Linear_Classifier(imdb4)
classifier4.grad_descent()

with open('linear_classifier4.pkl', 'wb') as f:
    cPickle.dump(classifier4,f,protocol=-1)

heatmap = classifier4.linear_weights[1:].reshape((32,32))
plt.imsave('Image_outputs/4_image_trained.png',heatmap)
hmap2 = plt.imshow(heatmap,cmap='gray', interpolation= 'None')
#plt.show(hmap2)
plt.close('all')
#################### Part 5 #######################################
print('Part 5')
make_data_set(['Fran Drescher', 'America Ferrera', 'Kristin Chenoweth', 'Alec Baldwin', 'Bill Hader', 'Steve Carell'],'5a',140)
make_data_set(['Fran Drescher', 'America Ferrera', 'Kristin Chenoweth', 'Alec Baldwin', 'Bill Hader', 'Steve Carell'],'5b',110)
make_data_set(['Fran Drescher', 'America Ferrera', 'Kristin Chenoweth', 'Alec Baldwin', 'Bill Hader', 'Steve Carell'],'5c',80)
make_data_set(['Fran Drescher', 'America Ferrera', 'Kristin Chenoweth', 'Alec Baldwin', 'Bill Hader', 'Steve Carell'],'5d',50)
make_data_set(['Fran Drescher', 'America Ferrera', 'Kristin Chenoweth', 'Alec Baldwin', 'Bill Hader', 'Steve Carell'],'5e',20)
make_data_set(['Gerard Butler', 'Daniel Radcliffe', 'Michael Vartan', 'Lorraine Bracco', 'Peri Gilpin', 'Angie Harmon'],'5t',20)
testing_data = Data_Base('training_set5t.txt','cropped/',{'Butler':-1, 'Radcliffe':-1,
                        'Vartan':-1, 'Bracco':1, 'Gilpin':1, 'Harmon':1})
val_data5 = Data_Base('training_set5a.txt','cropped/',{'Drescher':1,
                                                  'Ferrera':1,
                                                  'Chenoweth':1,
                                                  'Baldwin':-1,
                                                  'Hader':-1,
                                                  'Carell':-1})

ids = ['5a','5b','5c','5d','5e']
dbs = []
for id in ids:
    dbs.append(Data_Base('training_set%s.txt'%id,'cropped/',{'Drescher':1,
                                                  'Ferrera':1,
                                                  'Chenoweth':1,
                                                  'Baldwin':-1,
                                                  'Hader':-1,
                                                  'Carell':-1}))
i=0
training_res = np.ndarray(5)

#run regression
for db in dbs:
    classifier5 = Linear_Classifier(db)
    classifier5.grad_descent()
    #with open('linear_classifier%s-2.pkl'%ids[i], 'wb') as f:
    #    cPickle.dump(classifier5,f,protocol=-1)

    #with open('linear_classifier%s-2.pkl'%ids[i], 'rb') as f:
    #    classifier5 = cPickle.load(f)

    output_training = classifier5.forward()
    pred = np.zeros(output_training.shape, dtype=int)

    res = np.zeros(output_training.shape, dtype=int)
    gt_label = classifier5.y

    for j in range(0, output_training.shape[0]):
        if output_training[j] >= 0:
            pred[j] = 1
        else:
            pred[j] = -1
        res[j] = pred[j] == int(gt_label[j])

    accuracy = np.sum(np.array(res)) / float(len(res))
    print('Training accuracy: ')
    print(accuracy)
    training_res[i] = accuracy
    i+=1


plt.plot([140, 110, 80, 50, 20], training_res)
plt.ylim([0.9,1])
plt.xlabel('Number of Image per actor')
plt.ylabel('Accuracy on training set')
#plt.show()
plt.savefig('Image_outputs/training_res_part5.png')
plt.close('all')
#test classifiers
testing_results = np.ndarray((5,2))
j = 0
for id in ids:
    imdb_test = testing_data
    imdb_val = val_data5
    classifier5t = Linear_Classifier(imdb_test)
    classifier5v = Linear_Classifier(imdb_val)
    with open('linear_classifier%s-2.pkl'%id, 'rb') as f:
        classifier5o = cPickle.load(f)
    classifier5t.linear_weights = classifier5o.linear_weights.copy()
    classifier5v.linear_weights = classifier5o.linear_weights.copy()
    output_val = classifier5v.forward()
    output_test = classifier5t.forward()
    vloss_val = classifier5v.error(output_val)
    loss_val = classifier5t.error(output_test)
    print('Validation Loss')
    print(vloss_val)
    print('Testing Loss: ')
    print(loss_val)


    pred_val = np.zeros(output_val.shape, dtype=int)
    res = np.zeros(output_val.shape, dtype=int)
    gt_label = classifier5v.y
    for i in range(0, output_val.shape[0]):
        if output_val[i] >= 0:
            pred_val[i] = 1
        else:
            pred_val[i] = -1
        res[i] = pred_val[i] == int(gt_label[i])

    accuracyV = np.sum(np.array(res)) / float(len(res))
    print('Validation accuracy: ')
    print(accuracyV)

    pred_val = np.zeros(output_test.shape, dtype=int)
    res = np.zeros(output_test.shape, dtype=int)
    gt_label = classifier5t.y
    for i in range(0, output_test.shape[0]):
        if output_test[i] >= 0:
            pred_val[i] = 1
        else:
            pred_val[i] = -1
        res[i] = pred_val[i] == int(gt_label[i])

    accuracy = np.sum(np.array(res)) / float(len(res))
    print('Testing accuracy: ')
    print(accuracy)
    testing_results[j][0] = accuracyV
    testing_results[j][1] = accuracy
    j+=1

plt.plot([140, 110, 80, 50, 20], testing_results[:,0])
plt.plot([140, 110, 80, 50, 20], testing_results[:,1])
plt.xlabel('Number of Image per actor')
plt.ylabel('Accuracy')
plt.legend(['Validation','Testing'])
plt.savefig('N_im_vs_pref.png')
plt.close('all')
'''
Validation Loss
0.0556209862341
Testing Loss:
0.301930647316
Validation accuracy:
0.994047619048
Testing accuracy:
0.816666666667
Validation Loss
0.0747351836277
Testing Loss:
0.318667847575
Validation accuracy:
0.977380952381
Testing accuracy:
0.8
Validation Loss
0.103783873899
Testing Loss:
0.322492168105
Validation accuracy:
0.958333333333
Testing accuracy:
0.783333333333
Validation Loss
0.150479615249
Testing Loss:
0.417336222088
Validation accuracy:
0.929761904762
Testing accuracy:
0.683333333333
Validation Loss
0.21055522192
Testing Loss:
0.396036591325
Validation accuracy:
0.886904761905
Testing accuracy:
0.733333333333
'''

#################### Part 6 #######################################
print('Part 6')
#create new set of data
imdb6 = Data_Base('training_set5a.txt','cropped/',
                 {'Drescher':[1,0,0,0,0,0], 'Ferrera':[0,1,0,0,0,0], 'Chenoweth':[0,0,1,0,0,0],
                  'Baldwin':[0,0,0,1,0,0], 'Hader':[0,0,0,0,1,0], 'Carell':[0,0,0,0,0,1]})
#initialize classifier for n class classification
classifier6 = Linear_Classifier_nD(imdb6)
#obtain initial gradient
cl_grad = classifier6.grad(np.dot(classifier6.x, classifier6.linear_weights))
#random set of points to test for gradient computation
points = [[107,1],[0,0],[420,4],[329,2],[720,3],[185,5],[262,4],[638,0]]
#print out finite difference derivative alongside corresponding values of Grad
for point in points:
    h = np.zeros((1025, 6))
    h[point[0],point[1]] = 0.001
    error_a = classifier6.error(np.dot(classifier6.x, classifier6.linear_weights + h))
    error_b = classifier6.error(np.dot(classifier6.x, classifier6.linear_weights))
    true_deri = (error_a - error_b) / 0.001
    print "Manual Derivative: %f \tFrom Gradient: %f" %(true_deri, cl_grad[point[0],point[1]])

#################### Part 7 #######################################
print('Part 7-8')
#Perform gradient descent and save file for future use
classifier6.grad_descent()
'''
with open('classifier6.pkl','wb') as f:
    cPickle.dump(classifier6,f,protocol = -1)
'''
#with open('classifier6.pkl','rb') as f:
    #classifier6 = cPickle.load(f)

weights = classifier6.linear_weights.copy()

#Validation
val_data7 = Data_Base('validation_set5a.txt','cropped/',{'Drescher':[1,0,0,0,0,0], 'Ferrera':[0,1,0,0,0,0], 'Chenoweth':[0,0,1,0,0,0],
                  'Baldwin':[0,0,0,1,0,0], 'Hader':[0,0,0,0,1,0], 'Carell':[0,0,0,0,0,1]})

classifier7_val = Linear_Classifier_nD(val_data7)
classifier7_val.linear_weights = weights.copy()
outputs_val7 = classifier7_val.forward()
gt_label = classifier7_val.y

pred_val = np.zeros(outputs_val7.shape, dtype=int)
res = np.zeros(outputs_val7.shape[0], dtype=int)
for i in range(0,pred_val.shape[0]):
    pred_val[i,np.argmax(outputs_val7[i])] = 1
    res[i] = int(np.sum(pred_val[i]-gt_label[i]))==0

accuracy = np.sum(np.array(res)) / float(len(res))
print('Validation accuracy: ')
print(accuracy)

test_data7 = Data_Base('testing_set5a.txt','cropped/',{'Drescher':[1,0,0,0,0,0], 'Ferrera':[0,1,0,0,0,0], 'Chenoweth':[0,0,1,0,0,0],
                  'Baldwin':[0,0,0,1,0,0], 'Hader':[0,0,0,0,1,0], 'Carell':[0,0,0,0,0,1]})
classifier7_test = Linear_Classifier_nD(test_data7)
classifier7_test.linear_weights = weights.copy()
outputs_val7 = classifier7_test.forward()
gt_label = classifier7_test.y

pred_val = np.zeros(outputs_val7.shape, dtype=int)
res = np.zeros(outputs_val7.shape[0], dtype=int)
for i in range(0,pred_val.shape[0]):
    pred_val[i,np.argmax(outputs_val7[i])] = 1
    res[i] = int(np.sum(pred_val[i]-gt_label[i]))==0

accuracy = np.sum(np.array(res)) / float(len(res))
print('Testing Accuracy: ')
print(accuracy)

for j in range(0,6):
    heatmap = weights[1:,j].copy().reshape((32, 32))
    heatmap /=heatmap.max()
    heatmap +=1.
    #heatmap = plt.imshow(heatmap, cmap='heat', interpolation='None')
    #plt.show()
    hmap7 = plt.imsave('Image_outputs/actor_%i_of_6.png'%(j+1),heatmap,cmap='coolwarm')




